# -*- coding: utf-8 -*-
"""Image Recognition (VGG-16) .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NukjqgvC_4b06F_UidXXLW34_7BvUDsK
"""

!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d kritikseth/fruit-and-vegetable-image-recognition

!unzip fruit-and-vegetable-image-recognition.zip

train_dir='/content/train'
test_dir='/content/test'
valid_dir='/content/validation'

import torch
import torchvision
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from tqdm import tqdm
from sklearn.metrics import accuracy_score
import numpy as np
from PIL import Image
import time

BATCH_SIZE=64
EPOCHS=10
num_classes=36

train_transform=torchvision.transforms.Compose([
    torchvision.transforms.Resize((224,224)),
    torchvision.transforms.RandomCrop((224,224)),
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])

])


test_transform=torchvision.transforms.Compose([
    torchvision.transforms.Resize((224,224)),
    torchvision.transforms.CenterCrop((224,224)),
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])
])

train_dataset=torchvision.datasets.ImageFolder(train_dir,transform=train_transform)

test_dataset=torchvision.datasets.ImageFolder(test_dir,transform=test_transform)

valid_dataset=torchvision.datasets.ImageFolder(valid_dir,transform=test_transform)

train_loader=torch.utils.data.DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True)

test_loader=torch.utils.data.DataLoader(test_dataset,batch_size=BATCH_SIZE,shuffle=False)

valid_loader=torch.utils.data.DataLoader(valid_dataset,batch_size=BATCH_SIZE,shuffle=False)

class VGG16 (nn.Module):
  def __init__ (self,num_classes):
    super().__init__()

    self.block1=nn.Sequential(
        nn.Conv2d(3,64,kernel_size=(3,3),stride=(1,1),padding=1),
        nn.ReLU(),
        nn.Conv2d(64,64,kernel_size=(3,3),stride=(1,1),padding=1),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=(2,2),stride=(2,2))
    )


    self.block2=nn.Sequential(
        nn.Conv2d(64,128,kernel_size=(3,3),stride=(1,1),padding=1),
        nn.ReLU(),
        nn.Conv2d(128,128,kernel_size=(3,3),stride=(1,1),padding=1),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=(2,2),stride=(2,2))
    )


    self.block3=nn.Sequential(
        nn.Conv2d(128,256,kernel_size=(3,3),stride=(1,1),padding=1),
        nn.ReLU(),
        nn.Conv2d(256,256,kernel_size=(3,3),stride=(1,1),padding=1),
        nn.ReLU(),
        nn.Conv2d(256,256,kernel_size=(3,3),stride=(1,1),padding=1),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=(2,2),stride=(2,2))
    )


    self.block4=nn.Sequential(
        nn.Conv2d(256,512,kernel_size=(3,3),stride=(1,1),padding=1),
        nn.ReLU(),
        nn.Conv2d(512,512,kernel_size=(3,3),stride=(1,1),padding=1),
        nn.ReLU(),
        nn.Conv2d(512,512,kernel_size=(3,3),stride=(1,1),padding=1),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=(2,2),stride=(2,2))
    )

    self.block5=nn.Sequential(
        nn.Conv2d(512,512,kernel_size=(3,3),stride=(1,1),padding=1),
        nn.ReLU(),
        nn.Conv2d(512,512,kernel_size=(3,3),stride=(1,1),padding=1),
        nn.ReLU(),
        nn.Conv2d(512,512,kernel_size=(3,3),stride=(1,1),padding=1),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=(2,2),stride=(2,2))
    )

    self.classifier=nn.Sequential(
        nn.Linear(512*7*7,4096),
        nn.ReLU(True),
        nn.Dropout(p=0.5),
        nn.Linear(4096,4096),
        nn.ReLU(True),
        nn.Dropout(p=0.5),
        nn.Linear(4096,num_classes)
    )

    for m in self.modules():
      if isinstance(m,nn.Conv2d) or isinstance(m, nn.Linear):
        nn.init.kaiming_uniform_(m.weight,mode='fan_in',nonlinearity='relu')
        if m.bias is not None:
          m.bias.detach().zero_()


  def forward(self, x):
    x=self.block1(x)
    x=self.block2(x)
    x=self.block3(x)
    x=self.block4(x)
    x=self.block5(x)
    x=x.view(x.size(0),-1)
    x=self.classifier(x)
    return x

device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model=VGG16(36).to(device)

optimizer=optim.Adam(model.parameters(),lr=0.001)

criterion=nn.CrossEntropyLoss()

def accuracy(outputs,labels):
  _,predicted=torch.max(outputs,1)
  correct=(predicted==labels).sum().item()
  total=labels.size(0)
  return correct/total


def compute_accuracy(model,data_loader):
  model.eval()
  correct=0
  total=0
  with torch.no_grad():
    for inputs,labels in data_loader:
      inputs,labels=inputs.to(device),labels.to(device)
      outputs=model(inputs)
      _,predicted=torch.max(outputs,1)
      total += labels.size(0)
      correct += (predicted==labels).sum().item()
  return correct/total


def train (model,train_loader,optimizer,criterion):
  model.train()
  running_loss=0.0
  for inputs,labels in train_loader:
    inputs,labels=inputs.to(device),labels.to(device)
    optimizer.zero_grad()
    outputs=model(inputs)
    loss=criterion(outputs,labels)
    loss.backward()
    optimizer.step()
    running_loss += loss.item() * inputs.size(0)

  return running_loss/len(train_loader.dataset)


def validate(model,valid_loader,criterion):
  model.eval()
  running_loss=0.0
  with torch.no_grad():
    for inputs,labels in valid_loader:
      inputs,labels=inputs.to(device),labels.to(device)
      outputs=model(inputs)
      loss=criterion(outputs,labels)
      running_loss += loss.item() * inputs.size(0)

  return running_loss/len(valid_loader.dataset)

train_losses=[]
valid_losses=[]


for epoch in range(EPOCHS):
  start_time=time.time()
  train_loss=train(model,train_loader,optimizer,criterion)
  valid_loss=validate(model,valid_loader,criterion)
  train_accuracy=compute_accuracy(model,train_loader)
  valid_accuracy=compute_accuracy(model,valid_loader)

  end_time=time.time()
  epoch_time=end_time-start_time

  train_losses.append(train_loss)
  valid_losses.append(valid_loss)

  tqdm.write(f'Epoch {epoch+1}/{EPOCHS},'
             f'Train Loss: {train_loss:.4f},'
             f'Valid Loss: {valid_loss:.4f},'
             f'Train Accuracy: {train_accuracy:.4f},'
             f'Valid Accuracy: {valid_accuracy:.4f},/n'
             f'Time taken for Epoch:{epoch_time:.2f} seconds'

  )


plt.plot(train_losses,label='Train Loss')
plt.plot(valid_losses,label='Valid Loss')
plt.xlabel('Epochs')
plt.ylabel('loss')
plt.title('Train and Validation losses')
plt.legend()
plt.show()

